{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.vocab = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.char_to_int = { ch: i for i, ch in enumerate(self.vocab) }\n",
    "        self.int_to_char = { i: ch for i, ch in enumerate(self.vocab) }\n",
    "\n",
    "    def encode(self, s):\n",
    "        return [self.char_to_int[ch] for ch in s]\n",
    "\n",
    "    def decode(self, ints):\n",
    "        return ''.join(self.int_to_char[i] for i in ints)\n",
    "\n",
    "class TikTokenizer:\n",
    "    def __init__(self):\n",
    "        import tiktoken\n",
    "        self.enc = tiktoken.get_encoding('gpt2')\n",
    "        self.vocab = [self.enc.decode([i]) for i in range(self.enc.n_vocab)]\n",
    "        self.vocab_size = self.enc.n_vocab\n",
    "\n",
    "    def encode(self, s):\n",
    "        return self.enc.encode(s)\n",
    "\n",
    "    def decode(self, ints):\n",
    "        return self.enc.decode(ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicSplit:\n",
    "    def __init__(self, val_fraction=0.1):\n",
    "        self.val_fraction = val_fraction\n",
    "\n",
    "    def split(self, data):\n",
    "        n = int((1-self.val_fraction) * len(data))\n",
    "        train_data = data[:n]\n",
    "        val_data   = data[n:]\n",
    "        return train_data, val_data\n",
    "\n",
    "class ChunkSplit:\n",
    "    def __init__(self, val_fraction=0.1, num_chunks=12, encode=lambda x: x):\n",
    "\n",
    "        self.val_fraction = val_fraction\n",
    "        self.num_chunks   = num_chunks\n",
    "        self.encode       = encode\n",
    "\n",
    "    def split(self, data):\n",
    "        newline_token = self.encode(\"\\n\")\n",
    "\n",
    "        val_fraction = self.val_fraction\n",
    "        num_chunks   = self.num_chunks\n",
    "\n",
    "        total_len = data.size(0)\n",
    "        val_len   = int(val_fraction * total_len)\n",
    "        chunk_len = val_len // num_chunks\n",
    "        remainder = val_len % num_chunks\n",
    "\n",
    "        # split the entire dataset into chunks\n",
    "        chunks = []\n",
    "        idx = 0\n",
    "        while idx + chunk_len <= total_len:\n",
    "            chunks.append(data[idx : idx + chunk_len])\n",
    "            idx += chunk_len\n",
    "\n",
    "        # the leftover chunk (if any) after slicing out as many `chunk_len` blocks as possible. We always put this leftover into training by default\n",
    "        leftover = data[idx:]\n",
    "\n",
    "        val_indices = sorted(random.sample(range(len(chunks)), num_chunks))\n",
    "\n",
    "        # build the validation data (insert newline_token only between non-consecutive chunks)\n",
    "        val_data = []\n",
    "        prev_idx = -2  # something not adjacent to first pick \n",
    "        for i in val_indices:\n",
    "            # if the current chunk is NOT directly after the previous one and val_data already has content,\n",
    "            # then insert a newline token\n",
    "            if i != prev_idx + 1 and len(val_data) > 0:\n",
    "                val_data.append(torch.tensor(newline_token, dtype=torch.int64))\n",
    "            val_data.append(chunks[i])\n",
    "            prev_idx = i\n",
    "\n",
    "        # concatenate chosen val chunks\n",
    "        val_data = torch.cat(val_data) if val_data else torch.empty(0, dtype=torch.int64)\n",
    "\n",
    "        # if we have a remainder that is significant, tack data from train onto the end. This is ugly and bad and should be changed\n",
    "        # this ensures total validation tokens == val_len but can also result in a small amount of overlap (up to chunk_length -1 tokens) between val and train\n",
    "        if remainder > 100:\n",
    "            val_data = torch.cat((val_data, data[-remainder:]))\n",
    "\n",
    "        # everything else is training: the unchosen chunks plus leftover\n",
    "        train_indices = sorted(set(range(len(chunks))) - set(val_indices))\n",
    "        train_data_list = []\n",
    "        prev_idx = -2  # so first chunk won't auto-insert a newline\n",
    "        for i in train_indices:\n",
    "            # if this chunk is not consecutive to the previous one, insert a newline\n",
    "            if i != prev_idx + 1 and len(train_data_list) > 0:\n",
    "                train_data_list.append(torch.tensor(newline_token, dtype=torch.int64))\n",
    "            train_data_list.append(chunks[i])\n",
    "            prev_idx = i\n",
    "\n",
    "        train_data_list.append(torch.tensor(newline_token, dtype=torch.int64))\n",
    "        train_data_list.append(leftover)\n",
    "        train_data = torch.cat(train_data_list) if train_data_list else torch.empty(0, dtype=torch.int64)\n",
    "        return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file_path = \"datasets/tinyshakespeare.txt\"\n",
    "input_file_path = \"datasets/big.txt\"\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    train_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = CharTokenizer(train_text)\n",
    "tokenizer = TikTokenizer()\n",
    "encode = tokenizer.encode\n",
    "decode = tokenizer.decode\n",
    "vocab = tokenizer.vocab\n",
    "vocab_size = tokenizer.vocab_size\n",
    "val_fraction = 0.1\n",
    "splitter = ChunkSplit(val_fraction=0.1, num_chunks=16, encode = encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 2e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 512\n",
    "n_layer = 8\n",
    "n_head = 8\n",
    "dropout = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(train_text), dtype=torch.long)\n",
    "train_data, val_data = splitter.split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # lower-triangular mask for future tokens (causal attention)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # project x to key, query, value\n",
    "        k = self.key(x)    # (B, T, head_size)\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, T)\n",
    "        # mask out future positions for causal language modeling\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        # softmax to get attention weights\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        # dropout layer\n",
    "        wei = self.dropout(wei)\n",
    "        # weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "        out = wei @ v       # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        layers = []\n",
    "        for _ in range(n_layer):\n",
    "            layers.append(Block(n_embd, n_head=n_head))\n",
    "        \n",
    "        # Put all blocks in a Sequential container\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        \n",
    "        # final layer norm and output head\n",
    "        self.ln_f = nn.LayerNorm(n_embd)    # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C) or (B, T, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x) # apply a head of self attention\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 11.0000, val loss 11.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 500/5000 [02:41<18:53,  3.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 5.2896, val loss 5.5251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1000/5000 [05:18<16:34,  4.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 4.6263, val loss 4.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 1500/5000 [07:54<14:27,  4.03it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500: train loss 4.2385, val loss 4.7655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 2000/5000 [10:35<13:18,  3.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: train loss 3.9811, val loss 4.6446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 2500/5000 [13:19<10:41,  3.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500: train loss 3.7816, val loss 4.5798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 3000/5000 [16:03<09:12,  3.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000: train loss 3.5705, val loss 4.5191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 3500/5000 [18:47<06:30,  3.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3500: train loss 3.4328, val loss 4.5014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 4000/5000 [21:29<04:26,  3.75it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000: train loss 3.2461, val loss 4.4994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 4500/5000 [24:09<02:07,  3.94it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4500: train loss 3.1029, val loss 4.5082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 4999/5000 [26:52<00:00,  4.00it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4999: train loss 2.9519, val loss 4.5355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5000/5000 [27:25<00:00,  3.04it/s]\n"
     ]
    }
   ],
   "source": [
    "for iter in tqdm(range(max_iters), desc=\"Training\"):\n",
    "\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate From the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "This oak is bowed and it if someone were talking eagerly because they had a\n",
      "written, to crush her mistakes, her heart ever so before.\n",
      "\n",
      "The old count saw Frenchmen and still close on the table, which she\n",
      "steading a coquickerent beneath the\n",
      "marrowsy, several Count Bezukhov runghed, wrapped up her hands through the\n",
      "house. The child's hand timidly and she jumped\n",
      "and pressed it on the madiour either side of Peterearing the deep woman\n",
      "bounds. The footmen got ready yet the tea table. Anisya was Bogdanich\n",
      "asantly Baron.\n",
      "\n",
      "Anti-in sat, standing in a thirdident with a restrained\n",
      "tressmen approached the gold Preparer. Toward suddenly with prominent\n",
      "it Berg on his lipsore back, Nicholas, said, that it was not necessary to\n",
      "command under his excellency itself, nor should go back, to sleep about\n",
      "Pierre and now it all his movements.\n",
      "\n",
      "He looked round, as a scarf passed in a piece of blood without the cap\n",
      "suspending firing at the root of a small dog-blue scarf on\n",
      "round the table.\n",
      "\n",
      "\"Yes, General-diarrh!\" shouted a son of art' words she, pointing to\n",
      "Davout the door.\n",
      "\n",
      "The footman with a resolute limbs passed a mirror for the militia dance.\n",
      "But a statues smile was kept repeating. Another gave a look at hers\n",
      "into the lovers with water that seemed to about Peter Kirily requested. Five minutes later a\n",
      "general's keener, made the wifelled feet and thrust the trodden down--the smoke\n",
      "eated with a borzois wait, and darted. Makar Alexeevich\n",
      "billis gave grin at the porch on his stall, all sides of laughter\n",
      "where. But, behind the lighted whip and waved in his left hand sank\n",
      "and gazed intently on his neck on it and thenly.\n",
      "\n",
      "\"Shall Karagins!\" he cried.\n",
      "\n",
      "\"Who?\"\n",
      "\n",
      "\"Pierre replied that?\" said the prince. \"You seem to be glad if I\n",
      "brighter lose one--do not make the gyps?\"\n",
      "\n",
      "\"Yes, it can't say, he looks for it's sake... and they'll\n",
      "cake into the night,\" returned Prince Andrew.\n",
      "\n",
      "There were a whisper, multiple thoroughbred, the red-faced general, and\n",
      "turned let his legs under it.\n",
      "\n",
      "Prince Andrew, noticing that all looked intently face, but his face grew\n",
      "smiled fashion.\n",
      "\n",
      "\"What? Who is he?\" said this.\n",
      "\n",
      "\"Well?\" murmured Holmes. \"It's nothing, certainly the same, what sort state?\"\n",
      "\n",
      "\"Oh, look! Yes, master! You've came to his troops.\"\n",
      "\n",
      "And collected on the arms into the sofa by, with a cheerful smile; he relates\n",
      "understood the air of dogs appeared in his coldness.\n",
      "\n",
      "\"It's time!\" shouted the tall, grizzled old man, and brushed his face.\n",
      "Having gone up with a long\n",
      "showing under his uniform, passed by the open shouts, and with a hung on\n",
      "provisionence, he gazed at by, the speakerers and\n",
      "in the patient felt his jealousy that absorbed\n",
      "speaking the prince's sole and laughter again became enthusiastic. Seeing\n",
      "this he shook her head long and oppressed it at her, and\n",
      "gloved toward the calf quite\n",
      "inkled eyebrows and locked the assistant. Whether he seemed to her. \"To her\n",
      "heart, dear, 'Makeling Kell!\" she'll let the fox\n",
      "into her baby; he caught him a frightened face, and throwing up his\n",
      "down look.\n",
      "\n",
      "The door lit publican up so simple triumphantly, holding the veranda.\n",
      "\n",
      "But the opinion lit up by the pograms of Charch appeared people, and\n",
      "came out of the music was telling and musical in.\n",
      "\n",
      "At five minutes later in setting of the Rostovar Alexeevich, hunting-up-faced house\n",
      "with him with anger around him in the air by at the village.\n",
      " once the Elder came running in the crowd and his feverish\n",
      "beauty. He, trying to find out his relations with flushed and\n",
      "turning the rapturous respect of his fellow who outfluttered with shrinking\n",
      "which inviting him to cross the caleche by. A man seated himself, began looking\n",
      "round at the door, and with a look with a look of alert face uncovered.\n",
      "That evening, the stranger knew that his grief, Anna Pavlovna, that it hostess looked\n",
      "with spectacles, glanced round at a portrait and interrupting her on\n",
      "white teeth with trembling, bent with a deep sigh. But at Pierre, as it\n",
      "said that moment later it was ready and those they recalled it were the\n",
      "keys. In that while he frowned, nearer and puckering a young soldier's\n",
      "eyes had been fired at the altar. Looking the kerchief of the\n",
      "Davlograd hussar Alexeevich red.\n",
      "\n",
      "\"You see, but I have caught the name of Jerusalem in friendship,\" said\n",
      "Pierre, listening in a smile.\n",
      "\n",
      "Pierre did not consider what order happened about the meaning\n",
      " count to appear a family and a slight on his or understanding drew from\n",
      "the sitting room.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER III\n",
      "\n",
      "\n",
      "\"He cut everything upside on his hands.... Be! He thou will agree with her it,\n",
      "as if he does not want to go his charms. He is here by the news\n",
      "has given us; he has going again right to talk, but he alone...\" said the\n",
      "boding of irony that was too, the cheeks of a trunks\n",
      "terror as \"no feeling of it added in the same manner, and\n",
      "abandoned him to him, who was convinced that he appeared to look\n",
      "in what head did not forget the first remark.\n",
      "\n",
      "Anna Mikhaylovna's company told him that those\n",
      "consisted that that he was ready and was dying and that though he\n",
      "had to say so. On receiving what they was heard saying to say\n",
      "the princess's words Prince Andrew, when her friend said to him\n",
      "something still did not expect this brilliant sense, continued to deprive his son. Prince\n",
      "Andrew could not understand the more clearly be wondered at\n",
      "once, and secondly when outside himself had uttered some strain he suddenly\n",
      "talked at its training they would not run away Speranski's smilingly.\n",
      "\n",
      "A flush stole in his convoyman went over to his son's room,\n",
      "he suggested him that he had not\n",
      "seen him since the sixteenth of his wifeside, that besides his\n",
      "son, Helene had evidently not felt particularly pleased the\n",
      "master. He could go? She screamed her, her with understanding how she was said to\n",
      "beshe, and agitated to give her smile.\n",
      "\n",
      "Petya, quite surprised at the Rostovs' pretty, Rostov had promised to see the\n",
      "boy, when he tried to conceal it, and after still resounded with the\n",
      "brief features of the total. How she last was my friend's\n",
      "without inquiring? She could it behave\n",
      "she know of that word, nothing person is my wife, and is now so too.... At every\n",
      "love of her face said: \"Well, no one has\n",
      "well unjustly what I needed for. In this world, I'll tell you it it\n",
      "you, your only want to adlect she concluded and our trust,\"\n",
      "replied Nicholas, \"is here such a terrible trouble.\"\n",
      "\n",
      "Killed, however, his face pewantic role around him. \"The\n",
      "momentary an earth trembled in his nap, graceful, he is a great- reward\n",
      "and really does not refrain from under his\n",
      "his high, but self-nitsteringness...\" he said, with this it was also a face\n",
      "dimendure of wrath. The pleasant man turned away to his\n",
      "voice soon as he gazed at the beauty and a certain unnatural\n",
      "millions, the boy Mish's, then such as\n",
      "so jealous were accused, there arose in the manner\n",
      "of consciousness that were treason, in Nicholas, he began\n",
      "enure them to smile.\n",
      "\n",
      "The count returned and seal Sonya to know that they are wanted to hide\n",
      "Nicholas would not have any source of their pleasure, and it\n",
      "so. Victory soon saw they in order for dinner.\n",
      "\n",
      "Though attitude toward the count and Anna Pavlovna, who looked after\n",
      "his son's presence, but with animated by his reasons with\n",
      "that expression of that circle he recognized the first when the strong\n",
      "twoikhaysov round her.\n",
      "\n",
      "Since the letter met Countess Rostova's childish in Russia, Boris Drubetskoy\n",
      "Drubey from Julie that there is no\n",
      "man and sadnessself he must leave Moscow in every had of\n",
      "diplomatious study the best society: he or no of the army of been\n",
      "ably. Pierre, derived from the army above was Napoleon's\n",
      "terror and the count's head he iszukhov's. He was considering\n",
      "the buffoon, and only felt a position of the primitating\n",
      "terms, but themselves the future might find that he had never before to\n",
      "distouched their hopes at headquarters battle within him. But he did not need there so\n",
      "could not create this adjutant day to the kitchen someone else, the sooner\n",
      "would sacrifice they rushed in the\n",
      "fire handed of them--the position of the battle of Borodino and that\n",
      "hundred\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
