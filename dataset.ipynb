{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "!curl -o datasets/big.txt https://norvig.com/big.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 6488666 characters.\n"
     ]
    }
   ],
   "source": [
    "input_file_path = \"datasets/big.txt\"\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    train_text = f.read()\n",
    "\n",
    "print(f\"Loaded dataset with {len(train_text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of The Adventures of Sherlock Holmes\n",
      "by Sir Arthur Conan Doyle\n",
      "(#15 in our series by Sir Arthur Conan Doyle)\n",
      "\n",
      "Copyright laws are changing all over the world. Be sure to check the\n",
      "copyright laws for your country before downloading or redistributing\n",
      "this or any other Project Gutenberg eBook.\n",
      "\n",
      "This header should be the first thing seen when viewing this Project\n",
      "Gutenberg file.  Please do not remove it.  Do not change or edit the\n",
      "header without written permission.\n",
      "\n",
      "Please read the \"legal small print,\" and other information about the\n",
      "eBook and Project Gutenberg at the bottom of this file.  Included is\n",
      "important information about your specific rights and restrictions in\n",
      "how the file may be used.  You can also find out about how to make a\n",
      "donation to Project Gutenberg, and how to get involved.\n",
      "\n",
      "\n",
      "**Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
      "\n",
      "**eBooks Readable By Both Humans and By Computers, Since 1971**\n",
      "\n",
      "*****These eBooks Were Prepared By Thousan\n"
     ]
    }
   ],
   "source": [
    "print(train_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate unique chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_abcdefghijklmnopqrstuvwxyz|~\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(train_text)))\n",
    "num_unique_chars = len(chars)\n",
    "print(''.join(chars))\n",
    "print(num_unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a simple char tokenizeer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "char_to_int = { char:i for i,char in enumerate(chars) }\n",
    "int_to_char = { i:char for i,char in enumerate(chars) }\n",
    "encode = lambda str: [char_to_int[char] for char in str]\n",
    "decode = lambda ints: ''.join([int_to_char[i] for i in ints])\n",
    "\n",
    "print(decode(encode(\"hello world!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "encode = enc.encode\n",
    "decode = enc.decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6488666]) torch.int64\n",
      "tensor([54, 72, 69,  2, 50, 82, 79, 74, 69, 67, 84,  2, 41, 85, 84, 69, 78, 66,\n",
      "        69, 82, 71,  2, 39, 36, 79, 79, 75,  2, 79, 70,  2, 54, 72, 69,  2, 35,\n",
      "        68, 86, 69, 78, 84, 85, 82, 69, 83,  2, 79, 70,  2, 53, 72, 69, 82, 76,\n",
      "        79, 67, 75,  2, 42, 79, 76, 77, 69, 83,  1, 66, 89,  2, 53, 73, 82,  2,\n",
      "        35, 82, 84, 72, 85, 82,  2, 37, 79, 78, 65, 78,  2, 38, 79, 89, 76, 69,\n",
      "         1, 10,  5, 19, 23,  2, 73, 78,  2, 79])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(train_text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and validation sets based on randomly selected chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([54, 72, 69,  ..., 82, 16,  2]), tensor([53, 72, 69,  ..., 83, 85, 82]), tensor([1]), tensor([67, 65, 76,  ..., 84,  2, 79]), tensor([1]), tensor([73, 66, 76,  ..., 80, 65, 82]), tensor([1]), tensor([77, 69, 84,  ..., 78, 68,  2])]\n",
      "Training data size  : 5839801\n",
      "Validation data size: 648868\n",
      "Total data size     : 6488666\n"
     ]
    }
   ],
   "source": [
    "import torch, random\n",
    "\n",
    "newline_token = encode(\"\\n\")\n",
    "space_token = encode(\" \")\n",
    "\n",
    "val_fraction = 0.1\n",
    "num_chunks   = 5\n",
    "\n",
    "total_len = data.size(0)\n",
    "val_len   = int(val_fraction * total_len)\n",
    "chunk_len = val_len // num_chunks\n",
    "remainder = val_len % num_chunks\n",
    "\n",
    "# split the entire dataset into chunks\n",
    "chunks = []\n",
    "idx = 0\n",
    "while idx + chunk_len <= total_len:\n",
    "    chunks.append(data[idx : idx + chunk_len])\n",
    "    idx += chunk_len\n",
    "\n",
    "# the leftover chunk (if any) after slicing out as many `chunk_len` blocks as possible. We always put this leftover into training by default\n",
    "leftover = data[idx:]\n",
    "\n",
    "val_indices = sorted(random.sample(range(len(chunks)), num_chunks))\n",
    "\n",
    "# build the validation data (insert newline_token only between non-consecutive chunks)\n",
    "val_data = []\n",
    "prev_idx = -2  # something not adjacent to first pick \n",
    "for i in val_indices:\n",
    "    # if the current chunk is NOT directly after the previous one and val_data already has content,\n",
    "    # then insert a newline token\n",
    "    if i != prev_idx + 1 and len(val_data) > 0:\n",
    "        val_data.append(torch.tensor(newline_token, dtype=torch.int64))\n",
    "    val_data.append(chunks[i])\n",
    "    prev_idx = i\n",
    "\n",
    "print(val_data)\n",
    "\n",
    "# concatenate chosen val chunks\n",
    "val_data = torch.cat(val_data) if val_data else torch.empty(0, dtype=torch.int64)\n",
    "\n",
    "# if we have a remainder that is significant, tack data from train onto the end. This is ugly and bad and should be changed\n",
    "# this ensures total validation tokens == val_len but can also result in a small amount of overlap (up to chunk_length -1 tokens) between val and train\n",
    "if remainder > 100:\n",
    "    val_data = torch.cat((val_data, data[-remainder:]))\n",
    "\n",
    "# everything else is training: the unchosen chunks plus leftover\n",
    "train_indices = set(range(len(chunks))) - set(val_indices)\n",
    "train_data = [chunks[i] for i in train_indices]\n",
    "train_data.append(leftover)  # leftover always goes to training\n",
    "train_data = torch.cat(train_data) if train_data else torch.empty(0, dtype=torch.int64)\n",
    "\n",
    "print(\"Training data size  :\", train_data.size(0))\n",
    "print(\"Validation data size:\", val_data.size(0))\n",
    "print(\"Total data size     :\", total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([80, 82, 73, 83, 69, 16,  2, 35, 83, 84, 79, 78, 73, 83, 72, 77])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 15\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m context \u001b[38;5;241m=\u001b[39m x[:t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      5\u001b[0m target \u001b[38;5;241m=\u001b[39m y[t]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen input is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecode(context)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m the target: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(ints)\u001b[0m\n\u001b[0;32m      2\u001b[0m int_to_char \u001b[38;5;241m=\u001b[39m { i:char \u001b[38;5;28;01mfor\u001b[39;00m i,char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars) }\n\u001b[0;32m      3\u001b[0m encode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mstr\u001b[39m: [char_to_int[char] \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m ints: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([int_to_char[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ints])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(encode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello world!\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1].tolist()\n",
    "    target = [y[t].item()]\n",
    "    print(f\"when input is {decode(context)} the target: {decode(target)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
